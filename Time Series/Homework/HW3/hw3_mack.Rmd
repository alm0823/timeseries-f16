---
word_document:
  fig_height: 7
  fig_width: 10
author: "Justin Gomez and Andrea Mack"
date: ''
output: word_document
title: "Time Series HW 3"
---
__Disclaimer:__ Justin and Andrea are NEW partners, we have never worked together in one of Dr. Greenwood's classes on an assignment until now.
```{r settings, include=FALSE}
require(xtable)
require(effects)
require(car)#Anova
```

## HW 3

You can alone or in pairs that may or may not be people you worked with before. You can discuss it with old partners but should try work as much as possible with new collaborators. 5% bonus if you find someone completely new to work with - that you did not work with on first two assignments.

I mentioned de-seasonalizing of time series, where the seasonal variation is removed from the series to highlight variation at either higher or lower frequencies. There are a variety of techniques for doing this but the simplest is to just subtract the mean for each month from the observations. And the easiest way to find that is using `lm(y~month,data=...)`. 

1) For the Bozeman temperature data from HW 1 and 2, estimate a model with month only, subtract its fitted values from the responses (or just extract residuals). Plot the residuals vs the fractional `Year` variable and compare the plot of this result to the plot of the original time series. 

When plotting the original MMXT by fractional year, there just appears to be random scatter. The plot of the residuals reduces the noise and is suggestive of an upward positive trend in mean MMXT over time. The plot of the residuals makes potential outliers more apparent than in plotting the original MMXT by fractional year. 
```{r setup, include = FALSE}
rawbozemandata <- read.csv("https://dl.dropboxusercontent.com/u/77307195/rawbozemandata.csv", 
                           header = T)

rawd <- rawbozemandata
head(rawd)
rawd$year <- as.numeric(substr(rawd$DATE, 1,4))
rawd$monthf <- as.factor(month.abb[as.numeric(substr(rawd$DATE, 5,6))])
rawd$month <- as.numeric(substr(rawd$DATE, 5,6))
rawd$year.frac <- rawd$year + (rawd$month)/12
rawd$temp <- rawd$MMXT

lm1 <- lm(temp ~ monthf, data = rawd)

lm1.resid <- lm1$residuals
#options(show.sigif.stars = FALSE)
```

```{r prob1, fig.height = 5, fig.width = 5, fig.align = "center", eval = TRUE}
par(mfrow=c(2,1))
plot(lm1.resid ~ rawd$year.frac, xlab = "Fractional Year", ylab = "Residuals")

plot(MMXT ~ year.frac, data = rawd, xlab = "Fractional Year", ylab = "MMXT")

```

2) In the de-seasonalized Bozeman temperature data set, re-assess evidence for the linear trend. Compare the result (test statistic, degrees of freedom and size of p-value) of just fitting a linear time trend in these de-seasonalized responses to the result from our original model with a linear year component and a month adjustment (not the quadratic trend model).

CHECK IF THIS IS WHAT HE WANTS

\label{Deseasonalized Temperature Regressed on Year}
```{r prob2, results = 'asis'}
lm2 <- lm(lm1.resid ~ rawd$year)
options(show.signif.stars = FALSE)
lm2.sum <- summary(lm2)

xtable(lm2.sum)
```

\label{Temperature Regressed on Year and Month}
```{r prob2x, results = 'asis'}
lm3 <- lm(temp ~ year + monthf, data = rawd)
lm3.sum <- summary(lm3)

xtable(lm3.sum)
```

3) I briefly discussed the HADCRUT data set in class. We will consider the HADCRUT4 series of temperature anomalies for the Nothern Hemisphere. The fully up to date data set is available at: http://www.metoffice.gov.uk/hadobs/hadcrut4/data/current/time_series/HadCRUT.4.4.0.0.monthly_nh.txt

```{r prob3, include = FALSE}
hadcrut <- read.table("http://www.metoffice.gov.uk/hadobs/hadcrut4/data/current/time_series/HadCRUT.4.5.0.0.monthly_nh.txt", header = FALSE)
head(hadcrut)


hadcrut$date <- ts(hadcrut[,1], start = c(1850,01), end = c(2016,07), frequency = 12)
#only use ts if no months/years missing
```

Download the ensemble median monthly northern hemisphere temperature data. We will use the entire time series that is currently available (January 1850 to July 2016). You might want to read http://www.metoffice.gov.uk/hadobs/hadcrut4/data/current/series_format.html for more information on the columns in the data set.

Because the time series is complete over the time frame under consideration, you can use `ts()` to create a time variable instead of messing around with their `Year/Month` variable. 

Make a plot versus time of the ensemble medians and use that as your response variable in the following questions. Discuss trend, seasonality, outliers, and variability.

SO, to me the ensemble is the difference from the median temperature that is normal for that time of year. It's not clear to me whether the median is computed for each month before the difference is taken, or whether the overall median (regardless of month) is subtracted from each temperature value to get the ensemble variable.

DISCUSS

```{r prob3x}
#col2 are the ensemble medians

plot(hadcrut$V2 ~ hadcrut$date, xlab = "Months Since January 1850", ylab = "Ensemble Medians")

#an ensemble data set in which the 100 constituent
#ensemble members sample the distribution of likely surface temperature anomalies given our
#current understanding of these uncertainties
#Sea Surface Temperature anomalies in degrees Celsius, or "SST anomalies" for short, are how much temperatures depart from what is normal for that time of year.
```
4) Our main focus with these data will be on estimating the long-term trend, starting with polynomial trend models. But first, check for seasonality in a model that accounts for a linear time trend. Use our previous fractional year for the trend. Report an `effects` plot and a test for the month model component.
SEE COMMENT IN CODE CHUNK
ALSO, DO WE USE FRACTIONAL YEAR AS IN PART (A) OR DO WE USE TS OBJECT?

Based on an Fstat of 6.74 on 11 and 1986 degrees of freedom, with an associated pvalue of $\<$ 0.001, there is strong evidence that the effect of date on median ensemble changes by month.

```{r prob4, results = 'asis'}
hadcrut$monthf <- as.factor(month.abb[as.numeric(substr(hadcrut$V1,6,7))])
hadcrut$time.date <- time(hadcrut$date)
lm4 <- lm(V2 ~ time.date + monthf, data = hadcrut)#don't need monthf as explanatory bc month already induced in time??

plot(allEffects(lm4))

lm4.anova <- Anova(lm4, type = "II")
print(xtable(lm4.anova))

```
Note: when you use `time()` to generate the `Year` variable from a time series object it retains some time series object information that can cause conflicts later on. Create a new variable in your data.frame that uses something like `as.vector(time(tsdataname))`.

5) Check the residuals versus fitted values for any evidence of nonlinearity in the residuals vs fitted that was missed by the model with a linear trend and month component. Also note any potential issues with the constant variance assumption.

Viewing the residuals vs. fitted values plot below, there appears to be a quadratice relationship between the fitted values and residuals that was not captured by the model. For fitted values less than -0.3, there appears to be more variation in the residuals than for fitted values above -0.3. The model is less accurately predicting median ensembles when the original median ensemble was below -0.3, which to me means that we may have left out a variable or structure that explains when median ensemble was lower than expected.

```{r prob5}
par(mfrow = c(2,2))
plot(lm4)


```

6) You can add higher order polynomial terms to models using `x1+I(x1^2)+I(x1^3)`... or using the `poly` function, such as `poly(x1,3,raw=T)` for a cubic polynomial that includes the linear and quadratic components (we want this!). The `raw=T` keeps the variables in their raw or input format. Estimate the same model but now using polynomial trends that steps up from linear (poly(time,1,raw=T)) and stop when you get a failure to estimate a part of the model. Briefly discuss what happened.

When I got to the fifth degree polynomial, the estimate could not be computed. WHY? My guess is that it has something to do with the seasonality of the time.data variable, or it be perfectly collinear with another variable.
```{r prob6}
lm.d1 <- lm(V2~poly(time.date,1,raw=TRUE) + monthf, data = hadcrut)
summary(lm.d1)
lm.d <- NULL

for(i in 1:25){
lm.d[i] <- lm(V2~poly(time.date,i,raw=TRUE) + monthf, data = hadcrut)
}
lm.d[4]
lm.d[5]
```

7) If we center or, even better, make the polynomial functions orthogonal to one another, we can avoid the issue in the previous question. Use `poly(x1,?,raw=F)` and step up the polynomial order for time until the p-value for the last coefficient (use `summary()`) is "large", reporting the single test result for each step in the building process. Then drop back one order and re-fit the model. Report the `effects` plot of the resulting model and describe the estimated trend. Note: aside from access to orthogonal polynomials the `poly` function interfaces with `Anova` and the `effects` package really nicely.

8) Check the diagnostic plots from your final model. Does anything improve from the first version. Also plot the residuals vs time and compare that plot to residuals vs fitted.  


9) Run the following code so I can see what version of R you are now using:

### Documenting R version 

```{r}
getRversion()
```




