\documentclass[11pt]{article}
\usepackage{fullpage,graphicx,float,amsmath,enumitem,hyperref}
\setlist{parsep=5.5pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{\baselineskip}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Time Series Final Project}
\rhead{Andrea Mack}
\setlength{\headheight}{18pt}
\setlength{\headsep}{2pt}

\title{Time Series Final Project}
\author{Andrea Mack}
\date{November 29, 2016}

\begin{document}
\maketitle

Data were obtained. Of interest is properly modeling conditional volatility so as to make accurate predictions of conditional volatility. Conditional volatility is used in VaR to inform markets. Data were subsetted to include January 3, 1984 - December 31, 2015. Predictions will be made on 

The return($r_{t}$) in financial time series is defined by CC as:

$r_{t} = log(price_{t}) - log(price_{t-1})$

The return is thus define as the ratio of the log prices between two subsequent time points. The return is often then scaled by 100 to make numbers more easily thought about. 

Compare ARCH to iid and correlated errors.

<<setup, echo = FALSE, message = FALSE, cache = FALSE>>=
library(knitr)
opts_chunk$set(echo = FALSE, comment = NA, fig.align = "center",
               fig.width = 6.5, fig.height = 4, fig.pos = "H",
               size = "footnotesize", dev = "pdf",
               dev.args = list(pointsize = 11))
knit_theme$set("print")

require(xtable)
require(TSA)
require(ggplot2)
require(timeDate)
require(zoo)#yrmonth
require(fGarch)

options(xtable.table.placement = "H", xtable.caption.placement = "top",
        width = 80, scipen = 1, show.signif.stars = FALSE)
@


Volatility occurs when the conditional variance of a time series varies over time (CC, 279).

<<readin,include = FALSE>>=
oil <- read.csv("Cushing_OK_Crude_Oil_Future_Contract_1.csv")
colnames(oil) <- c("day", "price")
oil$day <- rev(oil$day)
oil$day <- as.character(oil$day)
head(oil$day)
tail(oil$day)


oil$date <- (as.Date(oil$day, format = "%m/%d/%Y"))
oil$r.price <- c(diff(log(oil$price))*100,NA)

# pull out 2016, will try to predict
y2016 <- grepl("2016", oil$day)
oil_past <- oil[-y2016,]
oil2016 <- oil[y2016,]


oil_sub <- oil[which(oil_past$day == "1/3/1984"):which(oil_past$day == "12/31/2015"),]

oil_sub$yearmn <- as.yearmon(oil_sub$day, "%m/%d/%Y")
plot(oil_sub$price)
price_yearmn <- by(oil_sub$price, oil_sub$yearmn, mean)
r.price_yearmn <- by(oil_sub$r.price, oil_sub$yearmn, mean)



# A 'timeDate' Sequence
#d1 <- oil$day[1]
#dlast <- oil$day[length(oil_sub$day)]
#tS <- timeSequence(as.Date("1983/4/4"), as.Date("2016/11/22"))
#tS

# Subset weekdays
#ggplot(data = oil_sub, aes(x = time(date), y = price)) + geom_line() + ylab("Price in Dollars") + xlab("Date") + theme_bw() + theme(legend.position = "bottom")

ggplot(data = oil_sub, aes(x = time(date), y = price)) + geom_line() + ylab("Price in Dollars") + xlab("Date") + theme_bw() + theme(legend.position = "bottom")


ggplot(data = oil_sub, aes(x = time(date), y = log(price))) + geom_line() + ylab("Log Price in Dollars") + xlab("Date") + theme_bw() + theme(legend.position = "bottom")

ggplot(data = oil_sub, aes(x = time(date), y = (r.price))) + geom_line() + ylab("Return in Dollars") + xlab("Date") + theme_bw() + theme(legend.position = "bottom")

@

The ACF and PACF plots inform about the correlation in returns at different time lags. The ACF of the daily returns suggest statistically significant autocorrelation at lags 2, 4, 5, and 8 with reoccuring patterns around day 30. The PACF of the daily returns suggests significant autocorrelation at lags 2, 3, 5, and 8 with again a couple reoccurant significant partial correlations at lags 29 and 33. The reoccuring significant lags after 30 days in both the ACF and PACF plots are not likely spurious as we have daily time series data (excluding holidays and weekends). The PACF adds information about correlations conditional on previous correlations. In the ACF plot lag 4 had a significant correlation whereas in the PACF plot it did not. Given the previous lags' autocorrelation, there is no evidence of additional autocorrelation at lag 4.

<<assess.autocorr>>=
par(mfrow=c(2,2))
acf(ts(data = oil_sub$r.price), main = "Returns in 100 Dollars")
pacf(ts(data = oil_sub$r.price), main = "Returns in 100 Dollars")


######################################################
#df_counts <- data.frame(table(oil_sub$yearmn))
#colnames(df_counts)[1] <- "yearmn"

#fn_rep <- function(x){
  #for()
  #rep(price_yearmn, df_counts)
#}

#ifelse(oil_sub$yearmn %*%)

#oil_sub$pricemn <- c(rep(0,dim(oil_sub)[1]))
#out <- NULL
#for(i in 1:length(tb_counts)){
  #out[[i]] <- list(rep(price_yearmn[i], tb_counts[i]))
#}

#require(dplyr)
#install.packages("data.table")
#require(data.table)

#c(paste(out))
@
Plotting non-linear transformations (versus linear as seen in correlation) of the returns aids in assessing whether observations at different time points are independent and identically distributed (CC 281). The ACF and PACF plots of the squared and absolute returns both have many signficant autocorrelations, suggesting observations at different time points are not independent and identically distributed.

<<abs.plots>>=
par(mfrow=c(2,2))
acf(ts(data = abs(oil_sub$r.price)), main = "Absolute Returns in 100 Dollars")
pacf(ts(data = abs(oil_sub$r.price)), main = "Absolute Returns in 100 Dollars")

acf(ts(data = (oil_sub$r.price)^2), main = "Squared Returns in 100 Dollars")
pacf(ts(data = (oil_sub$r.price)^2), main = "Squared Returns in 100 Dollars")
@

<<mli>>=
McLeod.Li.test(y=oil_sub$price, gof.lag=400)

McLeod.Li.test(y=oil_sub$r.price, gof.lag=400)

@

The McLeod-Li Test uses the squared returns so as above, is a test against serial independence. This can also be thought of as assuming no ARCH 

$H_{o}$: no ARCH

$H_{a}$: ARCH

The test statistic follows a $\chi^{2}_{m}$ distribution with m = the number of lags tested. \url{http://homepage.ntu.edu.tw/~ckuan/pdf/Lec-DiagTest.pdf} {\it could not find }

Both the daily prices and the returns suggest strong evidence of ARCH at all lags.

<<norm>>=
par(mfrow=c(2,2))
qqnorm(oil_sub$price, main = "NORMAL Q-Q Daily Prices")
qqline(oil_sub$price, col = "red")

qqnorm(oil_sub$r.price, main = "NORMAL Q-Q Returns")
qqline(oil_sub$r.price, col = "red")

@

The ACF and PACF of the returns suggest significant autocorrelations at the 5\% significance level. The ACF and PACF of the absolute value and squared returns both suggest independence is violated. Non-significant autocorrelation and signficiant serial dependence are commmon suggest volatility clustering. Volatility clustering visually is seen when the returns closer together display more similar variability. The Normal Q-Q plot of the returns shows heavy tails. Heavy tailed distributions and volatitlity clustering are common in financial time series data (CC 285). ARCH and GARCH models are used when data display these characteristics. 

Let the conditional variance of the returns, Var($r_{t|t-1}$) = $\sigma_{t|t-1}$. In ARCH modeling, a linear regression model of order q is fit to describe the relationship between the current conditional variance and the previous returns. 

$r_{t} = \sigma_{t|t-q}\epsilon_{t}$

$\sigma^{2}_{t|t-q} = \omega + \Sigma_{i=1:q} \alpha_{i}r^{2}_{t-i}$

such that $\epsilon_{t} \sim (0,1)$ and $\epsilon_{t} \perp r_{t-j}$ for j = 1,2,...

The conditional variance is not observable, and so with a linear transformation, the current squared return can be transformed to be a linear combination of previous squared returns and a random error such that $\r^{2}_{t}$ takes on the form of an AR(q) process.

$\r^{2}_{t} = \omega + \Sigma_{i=1:q} \alpha_{i}r^{2}_{t-i} + \Nu_{t}$

CC (286) show the weak stationarity assumption of AR processes holds for ARCH models, although the volatility clustering results in heavy tails not seen for AR processes with then white noise error terms are assumed to come from a Gaussian distribution. 

ARCH models are used to forcast conditional variances. 


The GARCH(p,q) model is a form of the ARMA(p,q) model such that the current conditional variance is a function of the p previous conditional variances and the q previous returns. Following the $\Nu_{t}$ transformation, the current squared return is written as a function of the max(p,q) previous squared returns and the p previous innovations ($\Nu_{t-p}$) and can be backtransformed to find the currect conditional variance (see CC 294 for the form). The GARCH model also exhibits weak stationarity and innovations from a heavy tailed distribution. However, CC (297) discuss an instance where the GARCH(1,1) model is nonstationary alone.

Parameter estimates are founding using ML.

\section*{Modeling with GARCH}

mark, how do I know what model to fit??? not sure from plots...
case where both acf of raw and squared etc shows sig implies arma and arch?
<<gmodels>>=

#garch -> order 1 is garch, order 2 is arch
# i think these should be fit based on plots

g121 <- garchFit(r.price=~arma(0,1)+garch(1,1),data=oil_sub,trace=FALSE,include.mean=FALSE)
summary(g121)
g121.aic <- AIC(g121)


g11 <- garch(oil_sub$r.price, order = c(1,1))
summary(g11)
g11.aic <- AIC(g11)

g8 <- garch(oil_sub$r.price, order = c(1,8))
summary(g8)
g8.aic <- AIC(g8)



g88 <- garch(oil_sub$r.price, order = c(8,8))
summary(g88)
g88.aic <- AIC(g88)

g48 <- garch(oil_sub$r.price, order = c(4,8))
summary(g48)
g48.aic <- AIC(g48)

oil_garch <- garchFit(oil_sub$r.price~garch(1,1))
summary(oil_garch)


oil_sub$date_numeric <- as.numeric(as.POSIXct(as.Date(oil_sub$date), format = "%m/%d/%Y"))

lm1 <- lm(oil_sub$r.price~oil_sub$date)
lm1.aic <- aic(lm1)[4]

aic_all <- data.frame(c(g8.aic,g121.aic, g11.aic, lm1.aic))
colnames(aic_all) <- "Model AIC"
rownames(aic_all) <- c("GARCH(8,1)","GARCH(12,1)", "GARCH(1,1)", "LM")

print(xtable(aic_all))
#would not fit timely
#arma1 <- gls(r.price~date, data = oil_sub, correlation = corARMA(p=1,q=1))
#aic(arma1)
@

The best fit model in terms of AIC was the GARCH(8,1) model. Normality and independence of squared residuals was tested for this model. CC estimate the long term variance as $\frac{\hat{\omega}}{1-\hat{\alpha} - \hat{\beta}}$. Below is a summary of the long term variances from all GARCH models considered, along with the sample long run variance in returns. All estimates are very close to that observed. {\it why would they be or not be?}

Note that garch uses a Quasi-Newton optimizer to find the ML estimates, for which AIC does not apply (cc 301).

<<compare.var>>=
r.obs_var <- var(oil_sub$r.price)
#eq cc300
g8_var <- coef(g8)[1]/(1-sum(coef(g8)[2:9]))
g121_var <- coef(g121)[1]/(1-sum(coef(g121)[2:14]))
g11_var <- coef(g121)[1]/(1-sum(coef(g11)[2:4]))

var_all <- data.frame(c(g8_var, g121_var, g11_var, r.obs_var))
colnames(var_all) <- "Long Run Variance Esitmates of Returns"
rownames(var_all) <- c("GARCH(8,1)","GARCH(12,1)", "GARCH(1,1)", "Sample")

print(xtable(var_all))
@


\section*{Assessing Model Assumptions}

The standardized errors are assumed to be independently and identically distributed.

CC Jarque-Bera test assesses normality, but Mark says not to use.
<<checking.assump>>=
par(mfrow=c(1,1))
qqnorm(residuals(g8, standardized = TRUE))
qqline(residuals(g8, standardized = TRUE), col = "red")

acf(residuals(g8, standardized = TRUE)^2, na.action=na.omit)
pacf(residuals(g8, standardized = TRUE)^2, na.action=na.omit)

@

{\it The assumptions look terrible.}

\section*{Predictions of 2016}

I have no idea what this is predicting

<<predict>>=
summary(g8)
predict(g8)

@

Still to do:
1) use comp code and apply to estimate var and pot
2) find other previous estimates and compare
3) discuss other garch model extensions and limitations

\section*{Extreme Value Theory and VaR}

Often of interest is modeling the extremes of the distribution of returns in a financial time series to inform potential risk. Often the exceedance of a threshold is used to model large returns (or losses) because it is shown to be more efficient than modeling the maxima (Charpentier, 258). Assuming the distribution of the returns, $x_{i}$ are independent and identically distributed with an unknown distribution F, Charpentier discusses the limiting ditribution of the $x_{i}$'s as converging in distribution to G under certain constraints with $\eta$ the extreme value index, the peak over threshold (POT) method introduced by (Charpentier, ) based in results from the extreme value theory theorem says that, ``when one selects a threshold high enough, the distribution of the values exceeding the threshold coverges in deistribution to the generalized Pareto distribution:

\begin{equation*}
$g_{\eta,\beta}(x)$ = \begin{cases}

$\frac{1}{\beta}(1+\eta x/\beta)^{-\frac{1}{\eta} -1}$ if $\eta \neq$ 0

$\frac{1}{\beta}exp({\frac{-x}{\beta}})$ if $\eta$ = 0
\end{cases}
\end{equation*}

for $\eta \leq 0$; x $\leq$ 0; $0 \geq x \geq -\beta/\eta$


Let Mn = the maximum of the distribution of returns

$G_{\eta}(x) = exp[-(1+(\eta)x)]$

Value at Risk (VaR) is the value that one might lose with a set probablity $\alpha$. Extreme value theory is related to modeling the tail of a distribution. 

<<var>>=

dgp <- function(x, beta = 1, xi=1){
  if(is.na(beta) || beta <= 0 || is.na(xi))
    return(rep(NA, length(x)))
  
  den <- numeric(length(x))
  
  # look for variates that are in the support range
  
  idx <-
    if(xi >= 0)
      x >= 0
  else
    (0 <= x & x <= -beta/xi)
  
  den[idx] <- 
    if(xi != 0)
      ((1+xi*x[idx]/beta)^(-1/xi-1))/beta
  else
    exp(-x[idx]/beta)/beta
  den}

dgp_oil <- dgp(oil_sub$r.price)

par(mfrow=c(1,1))
hist(dgp_oil)

evtVaR <- function(loss, alpha = 0.9, u = NULL){
  if(is.null(u))
    u <- quantile(loss, 0.95)
  y <- loss[loss > u] - u
  Nu <- length(y)
  n <- length(loss)
  fit <- nlminb(c(beta = 1, xi = 0),
                functon(par)
                -sum(log(dgp(y,par[1],par[2]))),
                lower = c(0.00000001, -Inf))
  beta <- fit$par[1]
  xi <- fit$par[2]
  VaR <- u + beta/xi * (((n/Nu)*(1-alpha))^(-xi) - 1)
                        names(VaR) <- NULL
                        VaR
}

@

\url{https://www.eia.gov/workingpapers/pdf/factors_influencing_oil_prices.pdf}

suggests that the cgarch model fits oil price data better than the garch model

account for long run price volatilities more

\url{https://www.r-bloggers.com/a-practical-introduction-to-garch-modeling/}

limitation may occur if markets change over time and combining all the markets into one data set

use the t distribution becaue of the fat tails

box-ljung test is not robust to extreme data, but the test is robust -> test whether autocorrelation is accounted for using a test on the ranks

persistence -- sum of first two output terms in garch model


CC use the {\texttt garch()} function from the __ package to fit GARCH models. That function is limited in that it does not allow fitting an ARMA and GARCH model at the same time. Since the crude oil returns were not modeled well by the GARCH model, the {\texttt rugarch} package was used as it allows more flexibility in both the GARCH model specified, as well as allowing available complexity with ARMA components.

{\it how do I get aic when I specify this way}

<<garch2>>=
spec1111 <- ugarchspec(variance.model = list(model = "sGARCH",
                                                                                  garchOrder = c(1, 1), external.regressors = NULL),
      mean.model= list(armaOrder = c(1, 1), include.mean = TRUE), distribution.model = "std") #student t
                                                                                  

oil_garch1111 <- ugarchfit(spec = spec1111, data = oil_sub$r.price, criterion = "AIC")
plot(oil_garch, which = 8)
plot(oil_garch, which = 9)
plot(oil_garch, which = 10)
plot(oil_garch, which = 11)


spec1122 <- ugarchspec(variance.model = list(model = "sGARCH",
                                                                                  garchOrder = c(2, 2), external.regressors = NULL),
      mean.model= list(armaOrder = c(1, 1), include.mean = TRUE), distribution.model = "std") #student t
                                                                                  

oil_garch1122 <- ugarchfit(spec = spec1122, data = oil_sub$r.price)
plot(oil_garch1122, which = 8)
plot(oil_garch1122, which = 9)
plot(oil_garch1122, which = 10)
plot(oil_garch1122, which = 11)

spec1111 <- ugarchspec(variance.model = list(model = "sGARCH",
                                                                                  garchOrder = c(1, 1), external.regressors = NULL),
      mean.model= list(armaOrder = c(1, 1), include.mean = TRUE), distribution.model = "std") #student t
                                                                                  

oil_garch1111 <- ugarchfit(spec = spec1111, data = oil_sub$r.price)
plot(oil_garch, which = 8)
plot(oil_garch, which = 9)
plot(oil_garch, which = 10)
plot(oil_garch, which = 11)


arma11 <- arima(oil_sub$r.price, order = c(1,1,1))
par(mfrow=c(2,2))
acf(residuals(arma11))
pacf(residuals(arma11))

acf(residuals(arma11)^2)
pacf(residuals(arma11)^2)

McLeod.Li.test(y=residuals(oil_garch1111), gof.lag=400)
McLeod.Li.test(y=residuals(oil_garch1111)^2, gof.lag=400)


@



{\it ask mark how to choose which model to use, if specifying right, and what more to do with this... actuarial code...?}



\end{document}
