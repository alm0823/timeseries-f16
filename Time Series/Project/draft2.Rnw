\documentclass[11pt]{article}
\usepackage{fullpage,graphicx,float,amsmath,enumitem,hyperref}
\setlist{parsep=5.5pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{\baselineskip}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Time Series Final Project}
\rhead{Andrea Mack}
\setlength{\headheight}{18pt}
\setlength{\headsep}{2pt}

\title{Time Series Final Project}
\author{Andrea Mack}
\date{November 29, 2016}

\begin{document}
\maketitle

Data were obtained. Of interest is properly modeling conditional volatility so as to make accurate predictions of conditional volatility. Conditional volatility is used in VaR to inform markets. Data were subsetted to include January 3, 1984 - December 31, 2015. Predictions will be made on 

<<setup, echo = FALSE, message = FALSE, cache = FALSE>>=
library(knitr)
opts_chunk$set(echo = FALSE, comment = NA, fig.align = "center",
               fig.width = 6.5, fig.height = 4, fig.pos = "H",
               size = "footnotesize", dev = "pdf",
               dev.args = list(pointsize = 11))
knit_theme$set("print")

require(xtable)
require(TSA)
require(ggplot2)
require(timeDate)
require(zoo)#yrmonth

options(xtable.table.placement = "H", xtable.caption.placement = "top",
        width = 80, scipen = 1, show.signif.stars = FALSE)
@


Volatility occurs when the conditional variance of a time series varies over time (CC, 279).

<<readin,include = FALSE>>=
oil <- read.csv("Cushing_OK_Crude_Oil_Future_Contract_1.csv")
colnames(oil) <- c("day", "price")
oil$day <- rev(oil$day)
oil$day <- as.character(oil$day)
head(oil$day)
tail(oil$day)


oil$date <- (format(as.Date(oil$day), "%d/%m/%Y"))
oil$r.price <- c(diff(log(oil$price))*100,NA)

# pull out 2016, will try to predict
y2016 <- grepl("2016", oil$day)
oil_past <- oil[-y2016,]
oil2016 <- oil[y2016,]


oil_sub <- oil[which(oil_past$day == "1/3/1984"):which(oil_past$day == "12/31/2015"),]

oil_sub$yearmn <- as.yearmon(oil_sub$day, "%m/%d/%Y")
plot(oil_sub$price)
price_yearmn <- by(oil_sub$price, oil_sub$yearmn, mean)
r.price_yearmn <- by(oil_sub$r.price, oil_sub$yearmn, mean)



# A 'timeDate' Sequence
#d1 <- oil$day[1]
#dlast <- oil$day[length(oil_sub$day)]
#tS <- timeSequence(as.Date("1983/4/4"), as.Date("2016/11/22"))
#tS

# Subset weekdays
#ggplot(data = oil_sub, aes(x = time(date), y = price)) + geom_line() + ylab("Price in Dollars") + xlab("Date") + theme_bw() + theme(legend.position = "bottom")

ggplot(data = oil_sub, aes(x = time(date), y = price)) + geom_line() + ylab("Price in Dollars") + xlab("Date") + theme_bw() + theme(legend.position = "bottom")


ggplot(data = oil_sub, aes(x = time(date), y = log(price))) + geom_line() + ylab("Log Price in Dollars") + xlab("Date") + theme_bw() + theme(legend.position = "bottom")

ggplot(data = oil_sub, aes(x = time(date), y = (r.price))) + geom_line() + ylab("Return in Dollars") + xlab("Date") + theme_bw() + theme(legend.position = "bottom")

@

The ACF and PACF plots inform about the correlation in returns at different time lags. The ACF of the daily returns suggest statistically significant autocorrelation at lags 2, 4, 5, and 8 with reoccuring patterns around day 30. The PACF of the daily returns suggests significant autocorrelation at lags 2, 3, 5, and 8 with again a couple reoccurant significant partial correlations at lags 29 and 33. The reoccuring significant lags after 30 days in both the ACF and PACF plots are not likely spurious as we have daily time series data (excluding holidays and weekends). The PACF adds information about correlations conditional on previous correlations. In the ACF plot lag 4 had a significant correlation whereas in the PACF plot it did not. Given the previous lags' autocorrelation, there is no evidence of additional autocorrelation at lag 4.

<<assess.autocorr>>=
par(mfrow=c(2,2))
acf(ts(data = oil_sub$r.price), main = "Returns in 100 Dollars")
pacf(ts(data = oil_sub$r.price), main = "Returns in 100 Dollars")


######################################################
#df_counts <- data.frame(table(oil_sub$yearmn))
#colnames(df_counts)[1] <- "yearmn"

#fn_rep <- function(x){
  #for()
  #rep(price_yearmn, df_counts)
#}

#ifelse(oil_sub$yearmn %*%)

#oil_sub$pricemn <- c(rep(0,dim(oil_sub)[1]))
#out <- NULL
#for(i in 1:length(tb_counts)){
  #out[[i]] <- list(rep(price_yearmn[i], tb_counts[i]))
#}

#require(dplyr)
#install.packages("data.table")
#require(data.table)

#c(paste(out))
@

ACF and PACF plots of non-linear transformations of the returns will help assess whether observations are independent and identically distributed, versus just having correlated observations (CC 281). There is strong evidence monthly log price returns are not independent and identically distributed. 

Idea: if lags in ts are independent, then independence will hold through (even) non-linear transformations, whereas if they are uncorrelated, *un*correlation will only hold through linear transformations.

<<abs.plots>>=
acf(ts(data = abs(oil_sub$r.price[-8403])))
pacf(ts(data = abs(oil_sub$r.price[-8403])))

acf(ts(data = (oil_sub$r.price[-8403])^2))
pacf(ts(data = (oil_sub$r.price[-8403])^2))
@

<<mli>>=
McLeod.Li.test(y=price_yearmn, gof.lag=length(price_yearmn))
McLeod.Li.test(y=oil_sub$price, gof.lag=400)

McLeod.Li.test(y=oil_sub$r.price[-8403], gof.lag=400)

@


Box-Ljung/McLeod-Li test

$H_{o}$: no ARCH

$H_{a}$: ARCH

Both the monthly and daily prices as well as the returns suggest strong evidence of ARCH at all lags.

<<norm>>=
qqnorm(price_yearmn, main = "NORMAL Q-Q Monthly Prices")
qqline(price_yearmn, col = "red")

qqnorm(oil_sub$price, main = "NORMAL Q-Q Daily Prices")
qqline(oil_sub$price, col = "red")


qqnorm(log(price_yearmn), main = "NORMAL Q-Q Monthly log(Prices)")
qqline(log(price_yearmn), col = "red")

qqnorm(log(oil_sub$price), main = "NORMAL Q-Q Daily log(Prices)")
qqline(log(oil_sub$price), col = "red")

qqnorm(oil_sub$r.price, main = "NORMAL Q-Q Returns")
qqline(oil_sub$r.price, col = "red")

@

The return($r_{t}$) in financial time series is defined by CC as:

$r_{t} = log(price_{t}) - log(price_{t-1})$

The return is thus define as the ratio of the log prices between two subsequent time points. The return is often then scaled by 100 to make numbers more easily thought about. The ACF and PACF of the returns suggest significant autocorrelations at the 5\% significance level. The ACF and PACF of the absolute value and squared returns both suggest independence is violated. Non-significant autocorrelation and signficiant serial dependence are commmon suggest volatility clustering. Volatility clustering visually is seen when the returns closer together display more similar variability. The Normal Q-Q plot of the returns shows heavy tails. Heavy tailed distributions and volatitlity clustering are common in financial time series data (CC 285). ARCH and GARCH models are used when data display these characteristics. 

Let the conditional variance of the returns, Var($r_{t|t-1}$) = $\sigma_{t|t-1}$. In ARCH modeling, a linear regression model of order q is fit to describe the relationship between the current conditional variance and the previous returns. 

$r_{t} = \sigma_{t|t-q}\epsilon_{t}$

$\sigma^{2}_{t|t-q} = \omega + \Sigma_{i=1:q} \alpha_{i}r^{2}_{t-i}$

such that $\epsilon_{t} \sim (0,1)$ and $\epsilon_{t} \perp r_{t-j}$ for j = 1,2,...

The conditional variance is not observable, and so with a linear transformation, the current squared return can be transformed to be a linear combination of previous squared returns and a random error such that $\r^{2}_{t}$ takes on the form of an AR(q) process.

$\r^{2}_{t} = \omega + \Sigma_{i=1:q} \alpha_{i}r^{2}_{t-i} + \Nu_{t}$

CC (286) show the weak stationarity assumption of AR processes holds for ARCH models, although the volatility clustering results in heavy tails not seen for AR processes with then white noise error terms are assumed to come from a Gaussian distribution. 

ARCH models are used to forcast conditional variances. 


The GARCH(p,q) model is a form of the ARMA(p,q) model such that the current conditional variance is a function of the p previous conditional variances and the q previous returns. Following the $\Nu_{t}$ transformation, the current squared return is written as a function of the max(p,q) previous squared returns and the p previous innovations ($\Nu_{t-p}$) and can be backtransformed to find the currect conditional variance (see CC 294 for the form). The GARCH model also exhibits weak stationarity and innovations from a heavy tailed distribution. However, CC (297) discuss an instance where the GARCH(1,1) model is nonstationary alone.

Parameter estimates are founding using ML.

\section*{Modeling with GARCH}
<<gmodels>>=

#garch -> order 1 is garch, order 2 is arch
# i think these should be fit based on plots
g11 <- garch(r.price_yearmn[-402], order = c(1,1))
summary(g11)
AIC(g11)

g22 <- garch(r.price_yearmn[-402], order = c(2,2))
summary(g22)
AIC(g22)

g05 <- garch(r.price_yearmn[-402], order = c(0,5))
summary(g05)
AIC(g05)


g012 <- garch(r.price_yearmn[-402], order = c(0,12))
summary(g012)
AIC(g012)


g121 <- garch(r.price_yearmn[-402], order = c(12,1))
summary(g121)
AIC(g121)


g121.d <- garch(oil_sub$r.price[-8403], order = c(12,1))
summary(g121.d)
AIC(g121.d)


g11.daily <- garch(oil_sub$r.price[-8403], order = c(1,1))
summary(g11.daily)
AIC(g11.daily)

oil_sub$date_numeric <- as.numeric(as.POSIXct(oil_sub$date, format = "%m/%d/%Y"))

lm1 <- lm(oil_sub$r.price[-8403]~as.numeric(as.character(oil_sub$date_numeric[-8403])))
aic(lm1)


@

Note that garch uses a Quasi-Newton optimizer to find the ML estimates, for which AIC does not apply (cc 301).

<<compare.var>>=
r.daily_var <- var(oil_sub$r.price[-8403])
r.monthly_var <- var(r.price_yearmn[-402])

#eq cc300
g121.daily_var <- coef(g121.d)[1]/(1-sum(coef(g121.d)[2:14]))
g121.monthly_var <- coef(g121)[1]/(1-sum(coef(g121.d)[2:14]))

@


\section*{Assessing Model Assumptions}

The standardized errors are assumed to be independently and identically distributed.

CC Jarque-Bera test assesses normality, but Mark says not to use.
<<checking.assump>>=
qqnorm(residuals(g121, standardized = TRUE))
qqline(residuals(g121, standardized = TRUE), col = "red")

acf(residuals(g121, standardized = TRUE)^2, na.action=na.omit)
#since some are still significant, that means violation of independence
gBox(g11, lags = c(1:20), x = r.price_yearmn[-402], method = "absolute")
@

Still to do:
1) use comp code and apply to estimate var and pot
2) find other previous estimates and compare
3) discuss other garch model extensions and limitations

\section*{Extreme Value Theory and VaR}

Often of interest is modeling the extremes of the distribution of returns in a financial time series to inform potential risk. Often the exceedance of a threshold is used to model large returns (or losses) because it is shown to be more efficient than modeling the maxima (Charpentier, 258). Assuming the distribution of the returns, $x_{i}$ are independent and identically distributed with an unknown distribution F, Charpentier discusses the limiting ditribution of the $x_{i}$'s as converging in distribution to G under certain constraints with $\eta$ the extreme value index, the peak over threshold (POT) method introduced by (Charpentier, ) based in results from the extreme value theory theorem says that, ``when one selects a threshold high enough, the distribution of the values exceeding the threshold coverges in deistribution to the generalized Pareto distribution:

\begin{equation*}
$g_{\eta,\beta}(x)$ = \begin{cases}

$\frac{1}{\beta}(1+\eta x/\beta)^{-\frac{1}{\eta} -1}$ if $\eta \neq$ 0

$\frac{1}{\beta}exp({\frac{-x}{\beta}})$ if $\eta$ = 0
\end{cases}
\end{equation*}

for $\eta \leq 0$; x $\leq$ 0; $0 \geq x \geq -\beta/\eta$


Let Mn = the maximum of the distribution of returns

$G_{\eta}(x) = exp[-(1+(\eta)x)]$

Value at Risk (VaR) is the value that one might lose with a set probablity $\alpha$. Extreme value theory is related to modeling the tail of a distribution. 

<<var>>=

dgp <- function(x, beta = 1, xi=1){
  if(is.na(beta) || beta <= 0 || is.na(xi))
    return(rep(NA, length(x)))
  
  den <- numeric(length(x))
  
  # look for variates that are in the support range
  
  idx <-
    if(xi >= 0)
      x >= 0
  else
    (0 <= x & x <= -beta/xi)
  
  den[idx] <- 
    if(xi != 0)
      ((1+xi*x[idx]/beta)^(-1/xi-1))/beta
  else
    exp(-x[idx]/beta)/beta
  den}
}

@

\url{https://www.eia.gov/workingpapers/pdf/factors_influencing_oil_prices.pdf}

suggests that the cgarch model fits oil price data better than the garch model

account for long run price volatilities more

\url{https://www.r-bloggers.com/a-practical-introduction-to-garch-modeling/}

limitation may occur if markets change over time and combining all the markets into one data set

use the t distribution becaue of the fat tails

box-ljung test is not robust to extreme data, but the test is robust -> test whether autocorrelation is accounted for using a test on the ranks

persistence -- sum of first two output terms in garch model







\end{document}
